{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e82c4e-7234-42aa-9727-b33627a2cf0a",
   "metadata": {},
   "source": [
    "# Lab 2 - Large Language Models + Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b67983-8857-4854-b05d-8e69edac17a9",
   "metadata": {},
   "source": [
    "### Task 1 - Index creation (15 points)\n",
    "\n",
    "- Set up a vector storage system. It can be any vector DBMS you want, or a SQL DBMS with vector support (for example, PostgreSQL with the [pgvector](https://github.com/pgvector/pgvector) add-on);\n",
    "- Set up an index management API. The API must contain the following endpoints:\n",
    "    - `/add_document` - accepts a document, splits it into chunks, vectorizes the chunks, and adds them to the vector storage;\n",
    "    - `/query` - accepts a query vector and returns the top K closest chunks.\n",
    "\n",
    "- Gather any document library you want. For example: scientific papers, lab reports from previous courses, etc.\n",
    "- Populate the vector storage with the gathered documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74dbe5c-617d-4073-9913-1ecef70772fd",
   "metadata": {},
   "source": [
    "### Task 2 - Q&A Assistant (15 points)\n",
    "\n",
    "- Select any LLM you want. Anything from [OpenAI](https://platform.openai.com/docs/overview), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Claude](https://claude.ai/), [Mistral](https://docs.mistral.ai/api/), to local ones like [Ollama](https://ollama.com/) or any model from [Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n",
    "\n",
    "- Create an LLM prompt template that contains:\n",
    "    - a task definition;\n",
    "    - placeholders for context chunks;\n",
    "    - a placeholder for the user query.\n",
    "\n",
    "- Set up the Q&A Assistant API. The API must contain:\n",
    "    - a `/query` endpoint, which accepts the user's question and returns the answer based on the retrieved documents.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
